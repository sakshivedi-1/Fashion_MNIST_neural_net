# Fashion-MNIST Neural Network Implementation

This repository contains a flexible, NumPy-based implementation of a Feedforward Neural Network built from scratch for the Fashion-MNIST classification task. This project was completed as part of the Atri Assignment to demonstrate understanding of Backpropagation, Gradient Descent variants, and hyperparameter tuning using Weights & Biases (Wandb).

## ðŸ“Œ Project Overview
The goal of this project is to classify images from the **Fashion-MNIST** dataset into 10 categories (e.g., T-shirt, Trouser, Bag) without using high-level frameworks like Keras/PyTorch for the model architecture.

### Key Features:
* **Built from Scratch:** No automatic differentiation or pre-built layers. All forward and backward passes are implemented using `numpy`.
* **Flexible Architecture:** Supports variable number of hidden layers and neuron counts.
* **Multiple Optimizers:** Implementation of 6 optimizers:
    * SGD
    * Momentum
    * Nesterov Accelerated Gradient (NAG)
    * RMSprop
    * Adam
    * Nadam
* **Experiment Tracking:** Integrated with **Wandb** for hyperparameter sweeps and visualization.
* **Activation Functions:** Support for ReLU, Sigmoid, and Tanh.

## ðŸš€ Getting Started

### Prerequisites
You need Python 3.x and the following libraries:
* `numpy`
* `pandas`
* `matplotlib`
* `wandb` (for experiment tracking)
* `scikit-learn` (for confusion matrix)
* `keras` (only for downloading the dataset)

### Installation
1.  **Clone the repository:**
    ```bash
    git clone [https://github.com/your-username/your-repo-name.git](https://github.com/your-username/your-repo-name.git)
    cd your-repo-name
    ```

2.  **Install dependencies:**
    ```bash
    pip install numpy pandas matplotlib wandb scikit-learn tensorflow
    ```

## ðŸƒâ€â™‚ï¸ How to Run

### Method 1: Google Colab (Recommended)
1.  Open the file `Atri_Assignment_FashionMNIST.ipynb` in Google Colab.
2.  **Enable GPU:** Go to `Runtime` > `Change runtime type` > Select `T4 GPU`.
3.  **Run Cells:** Execute the cells sequentially.
    * *Note:* You will be prompted to log in to your Wandb account in the first few cells. Copy your API key from [wandb.ai/authorize](https://wandb.ai/authorize) when asked.

### Method 2: Local Machine
1.  Ensure you have Jupyter Notebook installed.
2.  Launch the notebook:
    bash
    jupyter notebook Atri_Assignment_FashionMNIST.ipynb
  
3.  Run all cells.

## âš™ï¸ Usage & Configuration

### Training the Model
To train the model with specific hyperparameters, you can modify the `train()` function call or use the Wandb sweep configuration dictionary in the notebook.

**Example Configuration:**
python
config = {
    'epochs': 10,
    'batch_size': 32,
    'hidden_layers': 3,
    'hidden_size': 64,
    'learning_rate': 1e-3,
    'optimizer': 'nadam',
    'activation': 'relu',
    'weight_init': 'xavier',
    'weight_decay': 0.0005
}
Running Sweeps (Hyperparameter Tuning)
The notebook includes a sweep_config dictionary. To start a new sweep:

Define your parameter ranges in sweep_config (Question 4).

Run the wandb.sweep command provided in the notebook.

Use the generated sweep_id to start the agent: wandb.agent(sweep_id, train).

ðŸ“Š Results & Analysis
The project analyzes the performance of different configurations using Parallel Coordinates plots and Correlation matrices generated by Wandb.

Best Optimizer: (Update this after your run, e.g., Nadam)

Best Activation: (Update this, e.g., ReLU)

Evaluation: The final model is evaluated on the test set, and a confusion matrix is plotted to visualize class-wise performance.

ðŸ“ Assignment Requirements Checklist
[x] Question 1: Fashion-MNIST Data Visualization

[x] Question 2: Flexible Feedforward Neural Network Class

[x] Question 3: Backpropagation with 6 Optimizers

[x] Question 4: Wandb Hyperparameter Sweep

[x] Question 5: Accuracy Plots

[x] Question 6: Observations & Inferences

[x] Question 7: Confusion Matrix on Test Set

[x] Question 8: Squared Error vs. Cross Entropy Loss Comparison

[x] Question 10: MNIST Recommendations

ðŸ¤ Contributing
Feel free to fork this repository and submit pull requests. For major changes, please open an issue first to discuss what you would like to change.

ðŸ“œ License
MIT


---

### **2. How to Add and Update This File**

#### **Adding it to GitHub**
1.  Go to your repository on GitHub.
2.  Click the button **"Add file"** > **"Create new file"**.
3.  Name the file `README.md`.
4.  Paste the content above into the text editor.
5.  Click **"Commit changes"** at the bottom (green button).

#### **How to Update Information**
You will need to manually edit this file after you finish your experiments to make it accurate.

* **Replace Links:** Look for `https://github.com/your-username/your-repo-name.git` in the text above and replace it with your actual repository URL.
* **Update Results:** Under the **"Results & Analysis"** section, I left placeholders like `(Update this after your run, e.g., Nadam)`. Once you finish your Wandb sweep (from Day 4), delete my placeholder text and write the actual best optimizer you found.
    * *Example:* Change it to: `**Best Optimizer:** Nadam achieved the highest validation accuracy of 89%.`

#### **How to Update Code in the Repo**
If you change your Python code in Colab and need to update GitHub:
1.  In Colab, go to **File** > **Save a copy in GitHub**.
2.  It will ask for a "Commit message".
3.  Write something descriptive like "Fixed bug in backpropagation" or "Added confusion matrix plot".
4.  Click OK. This updates your code file (`.ipynb`) automatically. The `README.md` stays t
